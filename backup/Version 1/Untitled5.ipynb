{"cells":[{"cell_type":"markdown","metadata":{"id":"1mbRHo6hl5HA"},"source":["# Loading Libraries"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1709923642813,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"a40ukOOYkPv4"},"outputs":[],"source":["import sys\n","import os\n","import time\n","import argparse\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1709923642814,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"nus5pezlkZfi"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","from torch.autograd import Variable"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["\n","import craft_utils\n","import imgproc\n","import file_utils\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["\n","import json\n","import zipfile"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["import cv2\n","from skimage import io\n","import numpy as np"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["from PIL import Image"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1709923642816,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"UVCLkwpLkveD"},"outputs":[],"source":["from PIL import Image"]},{"cell_type":"markdown","metadata":{"id":"6yULtdSdpSbd"},"source":["#Loading Craft Model\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1709923665726,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"5eBodBa8mtpW"},"outputs":[],"source":["from craft import CRAFT"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1709923665726,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"Em6jLW4Qmsu8"},"outputs":[],"source":["from collections import OrderedDict\n","def copyStateDict(state_dict):\n","    if list(state_dict.keys())[0].startswith(\"module\"):\n","        start_idx = 1\n","    else:\n","        start_idx = 0\n","    new_state_dict = OrderedDict()\n","    for k, v in state_dict.items():\n","        name = \".\".join(k.split(\".\")[start_idx:])\n","        new_state_dict[name] = v\n","    return new_state_dict"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1709923665729,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"Xx7wgq9YnO_k"},"outputs":[],"source":["def str2bool(v):\n","    return v.lower() in (\"yes\", \"y\", \"true\", \"t\", \"1\")"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1709923665730,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"9gmLdmi4nQtR","outputId":"61b6a52b-1da6-4228-fa2a-c063777de327"},"outputs":[{"data":{"text/plain":["_StoreAction(option_strings=['--refiner_model'], dest='refiner_model', nargs=None, const=None, default='weights/craft_refiner_CTW1500.pth', type=<class 'str'>, choices=None, required=False, help='pretrained refiner model', metavar=None)"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["parser = argparse.ArgumentParser(description='CRAFT Text Detection')\n","parser.add_argument('--trained_model', default='weights/craft_mlt_25k.pth', type=str, help='pretrained model')\n","parser.add_argument('--text_threshold', default=0.7, type=float, help='text confidence threshold') #0.7\n","parser.add_argument('--low_text', default=0.4, type=float, help='text low-bound score')\n","parser.add_argument('--link_threshold', default=0.2, type=float, help='link confidence threshold') #0.2\n","parser.add_argument('--cuda', default=True, type=str2bool, help='Use cuda for inference') #Default is true to use gpu (gpu limit reached in colab)\n","parser.add_argument('--canvas_size', default=1280, type=int, help='image size for inference')\n","parser.add_argument('--mag_ratio', default=2.0, type=float, help='image magnification ratio') # 1.5\n","parser.add_argument('--poly', default=False, action='store_true', help='enable polygon type')\n","parser.add_argument('--show_time', default=False, action='store_true', help='show processing time')\n","parser.add_argument('--test_folder', default='Data2', type=str, help='folder path to input images')\n","parser.add_argument('--refine', default=False, action='store_true', help='enable link refiner')\n","parser.add_argument('--refiner_model', default='weights/craft_refiner_CTW1500.pth', type=str, help='pretrained refiner model')"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1709923665730,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"r-v21gWKnXW0"},"outputs":[],"source":["args, unknown = parser.parse_known_args()\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1709923665731,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"vvlUXEIqni4w"},"outputs":[],"source":["result_folder = './result/'\n","if not os.path.isdir(result_folder):\n","    os.mkdir(result_folder)"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1709923665731,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"Ywu0Kn9Lnd1Y"},"outputs":[],"source":["def test_net(net, image, text_threshold, link_threshold, low_text, cuda, poly, refine_net=None):\n","    t0 = time.time()\n","\n","    # resize\n","    img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(image, args.canvas_size, interpolation=cv2.INTER_LINEAR, mag_ratio=args.mag_ratio)\n","    ratio_h = ratio_w = 1 / target_ratio\n","\n","    # preprocessing\n","    x = imgproc.normalizeMeanVariance(img_resized)\n","    x = torch.from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n","    x = Variable(x.unsqueeze(0))                # [c, h, w] to [b, c, h, w]\n","    if cuda:\n","        x = x.cuda()\n","\n","    # forward pass\n","    with torch.no_grad():\n","        y, feature = net(x)\n","\n","    # make score and link map\n","    score_text = y[0,:,:,0].cpu().data.numpy()\n","    score_link = y[0,:,:,1].cpu().data.numpy()\n","\n","    # refine link\n","    if refine_net is not None:\n","        with torch.no_grad():\n","            y_refiner = refine_net(y, feature)\n","        score_link = y_refiner[0,:,:,0].cpu().data.numpy()\n","\n","    t0 = time.time() - t0\n","    t1 = time.time()\n","\n","    # Post-processing\n","    boxes, polys = craft_utils.getDetBoxes(score_text, score_link, text_threshold, link_threshold, low_text, poly)\n","\n","    # coordinate adjustment\n","    boxes = craft_utils.adjustResultCoordinates(boxes, ratio_w, ratio_h)\n","    polys = craft_utils.adjustResultCoordinates(polys, ratio_w, ratio_h)\n","    for k in range(len(polys)):\n","        if polys[k] is None: polys[k] = boxes[k]\n","\n","    t1 = time.time() - t1\n","\n","    # render results (optional)\n","    render_img = score_text.copy()\n","    render_img = np.hstack((render_img, score_link))\n","    ret_score_text = imgproc.cvt2HeatmapImg(render_img)\n","\n","    if args.show_time : print(\"\\ninfer/postproc time : {:.3f}/{:.3f}\".format(t0, t1))\n","\n","    return boxes, polys, ret_score_text"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1709923665731,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"BP-cq5jKn90o"},"outputs":[],"source":["def loadCraft():\n","    # load net\n","    global net\n","    net = CRAFT()     # initialize\n","\n","    print('Loading weights from checkpoint (' + args.trained_model + ')')\n","    if args.cuda:\n","        net.load_state_dict(copyStateDict(torch.load(args.trained_model)))\n","    else:\n","        net.load_state_dict(copyStateDict(torch.load(args.trained_model, map_location='cpu')))\n","\n","    if args.cuda:\n","        net = net.cuda()\n","        net = torch.nn.DataParallel(net)\n","        cudnn.benchmark = False\n","\n","    net.eval()\n","\n","    # LinkRefiner\n","    global refine_net\n","    refine_net = None\n","    if args.refine:\n","        from refinenet import RefineNet\n","        refine_net = RefineNet()\n","        print('Loading weights of refiner from checkpoint (' + args.refiner_model + ')')\n","        if args.cuda:\n","            refine_net.load_state_dict(copyStateDict(torch.load(args.refiner_model)))\n","            refine_net = refine_net.cuda()\n","            refine_net = torch.nn.DataParallel(refine_net)\n","        else:\n","            refine_net.load_state_dict(copyStateDict(torch.load(args.refiner_model, map_location='cpu')))\n","\n","        refine_net.eval()\n","        args.poly = True\n","\n","\n","\n","def runCraft():\n","    \n","    image_list, _, _ = file_utils.get_files(args.test_folder)\n","\n","    \n","\n","    t = time.time()\n","\n","    # load data\n","    for k, image_path in enumerate(image_list):\n","        print(\"Test image {:d}/{:d}: {:s}\".format(k+1, len(image_list), image_path), end='\\r')\n","        image = imgproc.loadImage(image_path)\n","\n","        bboxes, polys, score_text = test_net(net, image, args.text_threshold, args.link_threshold, args.low_text, args.cuda, args.poly, refine_net)\n","\n","        # save score text\n","        filename, file_ext = os.path.splitext(os.path.basename(image_path))\n","        mask_file = result_folder + \"/res_\" + filename + '_mask.jpg'\n","        cv2.imwrite(mask_file, score_text)\n","\n","        file_utils.saveResult(image_path, image[:,:,::-1], polys, dirname=result_folder)\n","\n","    print(\"elapsed time : {}s\".format(time.time() - t))"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5444,"status":"ok","timestamp":1709923671163,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"xsqauZq-p_iu","outputId":"3283a3f6-7ed4-4cd2-ab4f-98e1333a9c34"},"outputs":[{"name":"stderr","output_type":"stream","text":["e:\\Study\\Environments\\testlab\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","e:\\Study\\Environments\\testlab\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Loading weights from checkpoint (weights/craft_mlt_25k.pth)\n"]}],"source":["loadCraft()"]},{"cell_type":"markdown","metadata":{"id":"PMmThk9WoSjM"},"source":["#Loading TROCR"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16197,"status":"ok","timestamp":1709923693730,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"pIOHKC6loXd_","outputId":"a7746376-927f-41d5-b226-dd91e0f66057"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"]}],"source":["from transformers import VisionEncoderDecoderModel\n","from transformers import TrOCRProcessor\n","from PIL import Image, ImageFilter\n","model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n","processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")"]},{"cell_type":"markdown","metadata":{"id":"uSp2JCq7onEf"},"source":["#Preprocess Image"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1709923693731,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"-n9Z6OVXorWk"},"outputs":[],"source":["\n","\n","def calculate_threshold(boxes):\n","    vertical_gaps = []\n","    for i in range(1, len(boxes)):\n","        vertical_gap = boxes[i][1] - boxes[i-1][3]  \n","        vertical_gaps.append(vertical_gap)\n","    median_gap = np.median(vertical_gaps)\n","    threshold = median_gap * 5  # Adjust multiplier as needed\n","    return threshold\n","\n","def group_boxes_by_line(boxes, text_threshold):\n","    lines = []\n","    sorted_boxes = sorted(boxes, key=lambda x: (x[1], x[0]))\n","    current_line = [sorted_boxes[0]]\n","    for box in sorted_boxes[1:]:\n","        if box[1] - current_line[-1][1] < text_threshold:\n","            current_line.append(box)\n","        else:\n","            lines.append(current_line)\n","            current_line = [box]\n","    lines.append(current_line)\n","    return lines\n","def read_bounding_boxes_from_file(file_path):\n","    with open(file_path, 'r') as file:\n","        lines = file.readlines()\n","        print(lines)\n","        boxes = [list(map(int, line.strip().split(','))) for line in lines]\n","    return boxes\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["\n","\n","def crop_images(image_path,line_boxes):\n","    image = Image.open(image_path)\n","# Assuming `line_boxes` contains the line-level bounding boxes in the format (x_min, y_min, x_max, y_max)   \n","    #cropped_images = []\n","    result2=[]\n","    for i, line in enumerate(line_boxes):\n","        x_min = 9999\n","        y_min= 9999\n","        x_max=-9999\n","        y_max=-9999\n","\n","    for j, box in enumerate(line):\n","        # Assuming each box has 4 points\n","        x_min = min(box[0], box[2], box[4], box[6],x_min)\n","        y_min = min(box[1], box[3], box[5], box[7],y_min)\n","        x_max = max(box[0], box[2], box[4], box[6],x_max)\n","        y_max = max(box[1], box[3], box[5], box[7],y_max)\n","    result2.append((x_min,y_min,x_max,y_min,x_min,y_max,x_max,y_max))\n","    img_cropped=[]\n","    for i in range(len(result2)): #len(result2)-1\n","        x1, y1, x2, y2, x3, y3, x4, y4 = result2[i]\n","        #left = min(x1, x4)\n","        #upper = min(y1, y2)\n","        #right = max(x2, x3)\n","        #lower = max(y3, y4)\n","        img_cropped.append(image.crop((x1, y1, x4, y4)))\n","        for i in range (0,len(img_cropped)):\n","            img_cropped[i].save(f'Cropped/Test_Image{i}.jpg')\n","    return img_cropped\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["import re\n","\n","def remove_non_alphabet_characters(input_string):\n","    result_string = re.sub(r'[^a-zA-Z\\s]', '', input_string)\n","    return result_string\n","\n","\n","def TROCR(img_cropped,resultant_string=\"\"):\n","    for i in range (0,len(img_cropped)):\n","        image=Image.open(f'Cropped/Test_Image{i}.jpg')\n","        sharpened_img = image.filter(ImageFilter.SHARPEN)\n","        #processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n","        # calling the processor is equivalent to calling the feature extractor\n","        pixel_values = processor(sharpened_img, return_tensors=\"pt\").pixel_values\n","\n","        generated_ids = model.generate(pixel_values)\n","        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","        generated_text=remove_non_alphabet_characters(generated_text)\n","        generated_text=generated_text.strip()\n","        resultant_string+=generated_text.lower()\n","        resultant_string+=\" \"\n","        return resultant_string\n","    "]},{"cell_type":"markdown","metadata":{"id":"73EjWRA9mD-z"},"source":["# FAST API"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":565,"status":"ok","timestamp":1709924055161,"user":{"displayName":"Bilal Abbasi","userId":"12906580337326855029"},"user_tz":-300},"id":"GZzG9I4lPyaM"},"outputs":[],"source":["import os\n","import threading\n","import uvicorn\n","\n","from fastapi import FastAPI\n","from fastapi import File,UploadFile\n","\n","\n","app = FastAPI()\n","\n","\n","@app.get('/')\n","async def read_root():\n","    return \"Hello, FastAPI!\"\n","@app.post('/upload/')\n","async def image_upload(file: UploadFile = File(...), name: str = None):\n","    try:\n","        os.makedirs('Data2', exist_ok=True)\n","        file_path = os.path.join('Data2', file.filename)\n","        with open(file_path, 'wb') as f:\n","            contents = await file.read()\n","            f.write(contents)\n","    except Exception:\n","        return {\"message\": \"There was an error uploading the file\"}\n","    finally:\n","        file.file.close()\n","\n","    return {\"message\": f\"Successfully uploaded {file.filename}\"}\n","@app.get('/imageToText/')\n","async def imageToText(file: UploadFile = File(...)):\n","    try:\n","        runCraft()\n","        word_boxes = read_bounding_boxes_from_file(f'result/res_{file.filename}.txt')\n","        dynamic_threshold = calculate_threshold(word_boxes)\n","        line_boxes = group_boxes_by_line(word_boxes, dynamic_threshold)\n","        file_path = os.path.join('Data2', file.filename) ## change folder\n","        img_cropped=crop_images(file_path,line_boxes)\n","        \n","        resultant_string=TROCR(img_cropped)\n","        return resultant_string\n","        \n","        \n","        \n","    except Exception:\n","        return {\"message\": \"There was an error in converting image to handwritten Text\"}\n","        \n","        \n","    \n","    \n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","   "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNBuRapyozdfn8IgnP5DRC9","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
